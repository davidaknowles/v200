---
abstract: Assessing the severity of new pathogenic variants requires an understanding
  of which mutations enable escape of the human immune response. Even single point
  mutations to an antigen can cause immune escape and infection by disrupting antibody
  binding. Recent work has modeled the effect of single point mutations on proteins
  by leveraging the information contained in large-scale, pretrained protein language
  models (PLMs). PLMs are often applied in a zero-shot setting, where the effect of
  each mutation is predicted based on the output of the language model with no additional
  training. However, this approach cannot appropriately model immune escape, which
  involves the interaction of two proteins—antibody and antigen—instead of one protein
  and requires making different predictions for the same antigenic mutation in response
  to different antibodies. Here, we explore several methods for predicting immune
  escape by building models on top of embeddings from PLMs. We evaluate our methods
  on a SARS-CoV-2 deep mutational scanning dataset and show that our embedding-based
  methods significantly outperform zero-shot methods, which have almost no predictive
  power. We also highlight insights gained into how best to use embeddings from PLMs
  to predict escape. Despite these promising results, simple statistical and machine
  learning baseline models that do not use pretraining perform comparably, showing
  that computationally expensive pretraining approaches may not be beneficial for
  escape prediction. Furthermore, all models perform relatively poorly, indicating
  that future work is necessary to improve escape prediction with or without pretrained
  embeddings.
title: Predicting Immune Escape with Pretrained Protein Language Model Embeddings
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: swanson22a
month: 0
tex_title: Predicting Immune Escape with Pretrained Protein Language Model Embeddings
firstpage: 110
lastpage: 130
page: 110-130
order: 110
cycles: false
bibtex_author: Swanson, Kyle and Chang, Howard and Zou, James
author:
- given: Kyle
  family: Swanson
- given: Howard
  family: Chang
- given: James
  family: Zou
date: 2022-12-19
address:
container-title: Proceedings of the 17th Machine Learning in Computational Biology
  meeting
volume: '200'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 12
  - 19
pdf: https://proceedings.mlr.press/v200/swanson22a/swanson22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
